---
output_yaml:
  - _output.yml
---


<!-- To Do: -->

<!-- 1. Read chapter 5. Set up Zoom with Vivian. Don't you like her?  -->

<!-- 2. Read the latest version of themes.Rmd. Some of it is inspired but what was in the previous version of this chapter. But it is now more serious. So, it might be that we just cut everything here and start again. -->

<!-- Outline: -->

<!-- Preamble: Life is hard. Things were difficult in Chapter 9. Let us teach you some stuff to make your life easier! And you will get to practice in Chapters 11 and 12. Review themes.Rmd briefly, in two or three sentences. Review the key dificulties which were discussed in chapter 9: model selection, overfitting and causal inference. Finish with specific questions. A new professor is planning to teach a course. She knows the work she will assign and the enrollment. What will her Q scores be?   -->

<!-- 1. EDA of qscores. Keep all the rows for now. Change rating to rating*100. (That will make coefficients easy to deal with later.) -->

<!-- 2. rating ~ term + enrollment + hours. Do this first for just MUSIC classes. All with rstanarm. Fully explain.  All the way through posterior_linpred() and posterior_predict(). Add key items from style.Rmd, e.g., parameter uncertainty, unmodelled variation and so on. -->

<!-- 3. Explore the issue of which model to use. We did the above, and answered the question, using just data from Music courses. That seems sensible if it is a Music professor who asked the question. But isn't the other data relevant also? What would be the answer to her question if we used all the data to answer it? The answer will be different. Which is correct? Discuss. Key: There is not a single best model. Indeed, there is not a single best set of data to include. There is only a process, and the assumptions we make. -->

<!-- 4. Figuring out causal effects from observational data. Doing so stupidly. Doing so smart. We need a Rubin Causal Model section.  Set up a Zoom with Cass to discuss. She is a good source for information about how to make nice looking tables and about how to think about the issue. We might use qscores and just pretend/assume random assignment or we might use a different data set. Key is to set up a Rubin Table in which the potential outcomes are clear. For example, what would my rating be in a assigned 5 hours of work? What about 10 hours? 15 hours? Each of these is a differnt treatment and, therefore, generates a different potential outcome. -->

<!-- 5. The other key tool set is the one for tidymodels. We need a thorough discussion of them. I think we can put stan_glm within the tidymodels framework. So, do that! -->

<!-- 6. Use tidymodels framework to overfitting/underfitting and how we use bootstrap and cross-validation to fight against it.  -->

<!-- Other comments: -->

<!-- The key goal of this chapter is to walk students through all the most common pitfalls with creating models and then show them how to deal with those pitfalls. Then, in chapters 11 and 12, students put what we have taught them to work. The problem is that I only have a hazy idea of what the key pitfalls are and how to deal with them. -->


<!-- Type M/S errors. Seeing effects when none are there. -->


<!-- 5. Deriving causal estimates from observational data. -->

<!-- 6. Prediction only. rating ~ hours + department + enrollment + enrollment*course + whatever. Follow the Gelman advice on how to make a regression. What model do I make? Why? Think about goals? Teach tidymodels syntax. -->

<!-- 7. underfitting versus overfitting. Big idea. How to deal with? Bayes? Overfitting/underfitting. It would be nice to nail all the complexities of this down. Should we even bother to explore it in chapter 9? Holdout samples? -->

<!-- 8. Finish by answering question. -->


<!-- 

Tidymodels syntax. Again, get this settled before the "graduation exercise" of chapters 11/12.

M and S errors. This stuff is so important that it deserves placement in the main body of the book. Also, the governors data provides a great example to work with. Just use Gelman's blog post!

Estimating causal relationships which are somewhere between purely observational and perfect experiments: regression discontinuity designs, difference-in-difference and so on.

Dealing with parameter uncertainty correctly. That is, in previous chapters, we treated the betas as estimated perfectly. We did not incorporate uncertainty in their estimation into our prediction intervals. (Unless we did that with rstanarm in chapter 9. Or maybe this is a reason to save rstanarm for this chapter.)

Perhaps all this is enough to both fill this chapter and set the stage for 11 and 12.

-->


<!-- 6. We explicitly avoid talking about Bayesian models here because we are only estimating a one or two parameters in each of the models above. So, there is no occasion for pooling, or any of the other Bayesian magic. But you can see how estimating a 100 rating ~ hours models, one for each department, lends itself to a Bayesian approach. On to chapter 11. -->

<!-- Packages: tidyverse, broom -->

<!-- Commands:  -->

<!-- How do we solve the problems which were identified in chapter 9? -->

<!-- no bootstrap;  introduce tidymodels here for first time; -->

<!-- holdout sample, cross validation, machine learning, test  -->

<!-- DK: Maybe build this from tidymodels, while also mentioning the traditional way of just using lm()? The problem with raw lm() is that it does not work natively in a pipe since the first argument is a formula instead of the data. (Or maybe a lm(y ~ x, data = .) hack is OK?) -->

<!-- DK: Change the smoking example to political campaigns. If you work on the NYT, all you care about is forecasting election results conditional on campaign spending. If you are a (rich!) candidate, you care about the causal effect of spending on votes. Same model might be estimated by both! But the latter needs to be much more careful in deciding whether or not the results are real. -->

<!-- Albert points out a difficulty in combining the RCM with regression. You can't easily put in a distribution for the unknown potential outcome, even if you have a good regression model. You can't just add to the observed outcome because . . . actually I am confused about this!  -->

# Pitfalls {#pitfalls}

The fundamental goal of data modeling is to make explicit the relationship between:

* an *outcome variable* $y$, also called a *dependent variable* or response variable, and  
* an *explanatory/predictor variable* $x$, also called an *independent variable* or  covariate.

Another way to state this is using mathematical terminology: we will model the outcome variable $y$ "as a function" of the explanatory/predictor variable $x$. When we say "function" here, we aren't referring to functions in R like the `ggplot()` function, but rather to a mathematical function. But, why do we have two different labels, explanatory and predictor, for the variable $x$? That's because even though the two terms are often used interchangeably, roughly speaking data modeling serves one of two purposes:

1. **Modeling for explanation**: When you want to explicitly describe and quantify the relationship between the outcome variable $y$ and an explanatory variable $x$, determine the importance of any relationships, have measures summarizing these relationships, and possibly identify any *causal* relationships between the variables.  (What's a causal relationship? Remember the [Rubin Causal Model](#rubin-causal-model)! The *causal effect* of $x$ on $y$ is the difference in *potential outcomes* of $y$ given different values of $x$.)
1. **Modeling for prediction**: When you want to predict an outcome variable $y$ based on the information contained in a set of predictor variables $x$. Unlike modeling for explanation, however, you don't care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about $y$ using the information in $x$.

For example, say you are interested in an outcome variable $y$ of whether patients develop lung cancer and information $x$ on their risk factors, such as smoking habits, age, and socioeconomic status. If we are modeling for explanation, we would be interested in both describing and quantifying the effects of the different risk factors. One reason could be that you want to design an intervention to reduce lung cancer incidence in a population, such as increasing family income.  In that case, you would want to know the causal effect of income on the incidence of lung cancer.

If we are modeling for prediction, however, we wouldn't care so much about understanding how all the individual risk factors contribute to lung cancer, but rather only whether we can make good predictions of which people will contract lung cancer.

<!-- DK: Find a way to use this reference: [*An Introduction to Statistical Learning with Applications in R (ISLR)*](http://www-bcf.usc.edu/~gareth/ISL/)  -->

Linear regression involves a *numerical* outcome variable $y$ and explanatory variables $x$ that are either *numerical* or *categorical*. Furthermore, the relationship between $y$ and $x$ is assumed to be linear, or in other words, a line. However, we'll see that what constitutes a "line" will vary depending on the nature of your explanatory variables $x$.

<!-- DK: Could give a better plan overview, including discussion of chapters 11 and 12. Indeed, perhaps also looking backward to sampling and uncertainty. Need to rewrite this if we re-organize the book. Indeed, the introductions (and conclusions) to each chapter should be similar, providing a framework in which that chapter fits. -->

In Section \@ref(model1), the explanatory variable will be numerical. This scenario is known as *simple linear regression*. In Section \@ref(model2), the explanatory variable will be categorical.

In Chapter \@ref(continuous-response) on multiple regression, we'll extend the ideas behind basic regression and consider models with two explanatory variables $x_1$ and $x_2$.  In Section \@ref(model4), we'll have two numerical explanatory variables. In Section \@ref(model3), we'll have one numerical and one categorical explanatory variable. In particular, we'll consider two such models: *interaction* and *parallel slopes* models.

Let's now begin with basic regression, \index{regression!basic} which refers to linear regression models with a single explanatory variable $x$. We'll also discuss important statistical concepts like the *correlation coefficient*, that "correlation isn't necessarily causation," and what it means for a line to be "best-fitting."

Let's now load all the packages needed for this chapter (this assumes you've already installed them). The main packages are ones we have used before. The Advanced Section of the chapter makes use of

1. The **rstanarm** package, which provides an interface to the statistical inference engine, Stan, for Bayesian Regression Modeling. 
1. The **tidybayes** package, which aids in formating Bayesian modeling outputs in a tidy manner and provides ggplot geoms for plotting. 
1. The **broom.mixed** package, which provides broom-type functions for the output objects generated by **rstanarm**. 

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(PPBDS.data)
library(broom)
library(broom.mixed)
library(skimr)
library(gapminder)
library(rstanarm)
library(tidybayes)
```


```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in text.
library(mvtnorm)
library(kableExtra)
library(patchwork)
```


## Teaching evaluations: one numerical explanatory variable {#model1}


<!-- EG: I really like this section- I think that the in-depth explanations of not only how to find correlation coefficients but also interpret them accurately and effectively is great. -->

<!-- EG: I'll change this to qscores, along with an adjusted EDA for that dataset and more explanation of how correlation != causation. I'll also provide more investigation into the many ways confounding variables could impact why students provide certain qscores rather than simply hours of work, along with more language of comparison. -->


Why do some professors and instructors at universities and colleges receive high teaching evaluations scores from students while others receive lower ones? Are there differences in teaching evaluations between instructors of different demographic groups? Could there be an impact due to student biases? These are all questions that are of interest to university/college administrators, as teaching evaluations are among the many criteria considered in determining which instructors and professors get promoted.

In this section, we'll keep things simple for now and try to explain differences in instructor ratings within the Harvard music department based on average hourly workload for that class. Could it be that instructors with lower hourly workloads also have higher ratings? Could it be instead that instructors with lower hourly workloads tend to have lower ratings? Or could it be that there is no relationship between workload and teaching evaluations? We'll answer these questions by modeling the relationship between rating and workload using *simple linear regression* \index{regression!simple linear} where we have:

1. A numerical outcome variable $y$ (the instructor's teaching rating) and
1. A single numerical explanatory variable $x$ (the average hourly workload for the class).

### Exploratory data analysis {#model1EDA}

The data on the Q Guide Music Department ratings can be found in the `qscores` data frame included in the **PPBDS.data** package. However, to keep things simple, let's `select()` only the subset of the variables we'll consider in this chapter, and save this data in a new data frame called `qscores_ch10`:

```{r}
library(PPBDS.data)

qscores_ch10 <- qscores %>%
  mutate(rating = rating*100) %>%
  filter(department == "MUSIC") %>%
  select(number, rating, hours, enrollment)

```

A crucial step before doing any kind of analysis or modeling is performing an *exploratory data analysis*, \index{data analysis!exploratory} or EDA for short. EDA gives you a sense of the distributions of the individual variables in your data, whether any potential relationships exist between variables, whether there are outliers and/or missing values, and (most importantly) how to build your model. Here are three common steps in an EDA:

<!-- DK: Good stuff. We should keep this and follow it, each chapter. -->

1. Most crucially, looking at the raw data values.
1. Computing summary statistics, such as means, medians, and interquartile ranges.
1. Creating data visualizations.

Let's perform the first common step in an exploratory data analysis: looking at the raw data values. Because this step seems so trivial, unfortunately many data analysts ignore it. However, getting an early sense of what your raw data looks like can often prevent many larger issues down the road. 

You can do this by using RStudio's spreadsheet viewer or by using the `glimpse()` function as introduced in Subsection \@ref(exploredataframes) on exploring data frames:

<!-- DK: Add summary() -->

```{r}
glimpse(qscores_ch10)

glimpse(qscores_ch10) %>%
  summary()

```

<!-- EG: Should I just keep glimpse() with summary(), or should I include an analysis of glimpse() alone before doing both? -->

Observe that `Rows: 14` indicates that there are 14 rows/observations in `qscores_ch10`, where each row corresponds to one observed music course at Harvard. It is important to note that the *observational unit* \index{observational unit} is an individual course and not an individual instructor. Recall from Subsection \@ref(exploredataframes) that the observational unit is the "type of thing" that is being measured by our variables. Since instructors teach more than one course in an academic year, the same instructor will appear more than once in the data. Hence there are fewer than 748 unique instructors being represented in `qscores_ch10`.

To further explore the data, we can add a second function to our initial call of `glimpse()`: the `summary()` function. `summary()` can provide us with useful result summaries of all the observations in a dataset for each variable. Calling `glimpse()` on `qscores_ch10` and then `summary()` allows us to see the minimum and maximum values along with several other quantiles for numeric variables and tells us the number of observations, class, and mode of categorical variables. For instance, examining the `hours` column in the `summary()` results shows that the minimum number of workload hours reported for a Harvard music class in the Q Guide was 2.5 hours, while the median was 3.5 hours and the maximum was 5.2 hours.

<!-- EG: Commenting this out for the moment "A full description of all the variables included in `evals` can be found at [openintro.org](https://www.openintro.org/data/index.php?data=evals) or by reading the associated help file (run `?evals` in the console)." Is there a place where we have descriptions of the PPBDS library data like this? --> 

However, let's fully describe only the `r ncol(qscores_ch10)` variables we selected in `qscores_ch10`:

1. `number`: An identification variable used to distinguish among courses within the same department. Courses in different departments may have the same number.
1. `rating`: A numerical variable of the overall quality of a course, where the average is computed from the evaluation scores from all the students who choose to provide feedback for that course.  Ratings of 1 are lowest and 5 are highest. This is the outcome variable $y$ of interest.
1. `hours`: A numerical variable of the amount of work students put into a course per week in hours, where the average is computed from the evaluation scores from all students who choose to provide feedback for that course. This is the explanatory variable $x$ of interest.
1. `enrollment`: A numerical variable of the amount of students enrolled in a course.

An alternative way to look at the raw data values is by choosing a random sample of the rows in `qscores_ch10` by piping it into the `sample_n()` \index{dplyr!sample\_n()} function from the **dplyr** package. Here we set the `size` argument to be `5`, indicating that we want a random sample of 5 rows. We display the results below. Note that due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.

```{r, eval=FALSE}
qscores_ch10 %>%
  sample_n(size = 5)
```

```{r, echo=FALSE, fig.cap="A random sample of 5 out of the 748 courses at Harvard"}
qscores_ch10 %>%
  sample_n(5) 
```

<!-- EG: Should we still include summarize() if we're including summary() with glimpse() above? Summary() seems to include more information and is generally more useful. Or should we simply compare the two and advise when one could be more useful than the other? -->

Now that we've looked at the raw values in our `qscores_ch10` data frame and got a preliminary sense of the data, let's move on to the next common step in an exploratory data analysis: computing summary statistics. Let's start by computing the mean and median of our numerical outcome variable `rating` and our numerical explanatory variable `hours`. We'll do this by using the `summarize()` function from `dplyr` along with the `mean()` and `median()` summary functions we saw in Section \@ref(summarize).

```{r, eval=TRUE}
qscores_ch10 %>%
  summarize(mean_hours = mean(hours),
            mean_rating = mean(rating),
            median_hours = median(hours),
            median_rating = median(rating))
```

<!-- DK: This is nice. Having motivated the use of skim() once, we can just go straight to using it in other chapters. And/or show other tricks each chapter, like across(). -->

However, what if we want other summary statistics as well, such as the standard deviation (a measure of spread), the minimum and maximum values, and various percentiles? 

Typing out all these summary statistic functions in `summarize()` would be long and tedious. Instead, let's use the convenient `skim()` function from the **skimr** package. This function takes in a data frame, "skims" it, and returns commonly used summary statistics. Let's take our `qscores_ch10` data frame, `select()` only the outcome and explanatory variables teaching `score` and `hours`, and pipe them into the `skim()` function:

```{r}
qscores_ch10 %>% 
  select(rating, hours) %>% 
  skim()
```

For the numerical variables teaching `rating` and `hours` it returns:

- `n_missing`: the number of missing values
- `complete_rate`: the percentage of non-missing or complete values
- `mean`: the average
- `sd`: the standard deviation
- `p0`: the 0th percentile: the value at which 0% of observations are smaller than it (the *minimum* value)
- `p25`: the 25th percentile: the value at which 25% of observations are smaller than it (the *1st quartile*)
- `p50`: the 50th percentile: the value at which 50% of observations are smaller than it (the *2nd* quartile and more commonly called the *median*)
- `p75`: the 75th percentile: the value at which 75% of observations are smaller than it (the *3rd quartile*)
- `p100`: the 100th percentile: the value at which 100% of observations are smaller than it (the *maximum* value)

Looking at this output, we can see how the values of both variables are distributed. For example, the mean music course rating was 4.41 out of 5, whereas the mean workload per week was 3.46 hours. Furthermore, the middle 50% of course ratings was between 4.2 and 4.8 (the first and third quartiles), whereas the middle 50% of hours of work falls within 2.87 to 3.7, with a maximum reported workload of 5.2 hours per week.

<!-- DK: Keep this. -->

The `skim()` function only returns what are known as *univariate* \index{univariate} summary statistics: functions that take a single variable and return some numerical summary of that variable. However, there also exist *bivariate* \index{bivariate} summary statistics: functions that take in two variables and return some summary of those two variables. In particular, when the two variables are numerical, we can compute the \index{correlation (coefficient)} *correlation coefficient*. Generally speaking, *coefficients* are quantitative expressions of a specific phenomenon.  A *correlation coefficient* is a quantitative expression of the *strength of the linear relationship between two numerical variables*. Its value ranges between -1 and 1 where:

* -1 indicates a perfect *negative relationship*: As one variable increases, the value of the other variable tends to go down, following a straight line.
* 0 indicates no relationship: The values of both variables go up/down independently of each other.
* +1 indicates a perfect *positive relationship*: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion.

The following figure gives examples of 9 different correlation coefficient values for hypothetical numerical variables $x$ and $y$. For example, observe in the top right plot that for a correlation coefficient of -0.75 there is a negative linear relationship between $x$ and $y$, but it is not as strong as the negative linear relationship between $x$ and $y$ when the correlation coefficient is -0.9 or -1.

```{r, echo=FALSE, fig.cap="Nine different correlation coefficients."}
correlation <- c(-0.9999, -0.9, -0.75, -0.3, 0, 0.3, 0.75, 0.9, 0.9999)
n_sim <- 100
values <- NULL
for(i in seq_along(correlation)){
  rho <- correlation[i]
  sigma <- matrix(c(5, rho * sqrt(50), rho * sqrt(50), 10), 2, 2)
  # EG- should we consider explaining more about what is happening within "sim"? I personally haven't used rmvnorm before and am unsure whether it's been discussed in a previous chapter, or whether some explanation/clarification could be helpful.
  sim <- rmvnorm(
    n = n_sim,
    mean = c(20,40),
    sigma = sigma
    ) %>%
    as.data.frame() %>% 
    as_tibble() %>%
    mutate(correlation = round(rho,2))

  values <- bind_rows(values, sim)
}

corr_plot <- ggplot(data = values, mapping = aes(V1, V2)) +
  geom_point() +
  facet_wrap(~ correlation, ncol = 3) +
  labs(x = "x", y = "y") +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks = element_blank())

if(knitr::is_latex_output()){
  corr_plot +
  theme(
    strip.text = element_text(colour = 'black'),
    strip.background = element_rect(fill = "grey93")
  )
} else {
  corr_plot
}
```


The correlation coefficient can be computed using the `cor()` summary function within a `summarize()`:

```{r, eval=FALSE}
qscores_ch10 %>% 
  summarize(correlation = cor(rating, hours))
```

```{r, echo=FALSE}
cor_ch10 <- qscores_ch10 %>%
  summarize(correlation = cor(rating, hours)) %>% 
  round(3) %>% 
  pull()
```

In our case, the correlation coefficient of `r cor_ch10` indicates that the relationship between overall course rating and average weekly workload in hours is negative. There is a certain amount of subjectivity in interpreting correlation coefficients, especially those that aren't close to the extreme values of -1, 0, and 1. 

Let's now perform the last of the steps in an exploratory data analysis: creating data visualizations. Since both the `rating` and `hours` variables are numerical, a scatterplot is an appropriate graph to visualize this data. Let's do this using `geom_point()` and display the result. Furthermore, let's highlight the six points in the top right of the visualization in a box.

```{r, eval=FALSE}
qscores_ch10 %>%
  ggplot(aes(x = hours, y = rating)) +
  geom_point() +
  labs(x = "Hours of Work Per Week", 
       y = "Q Guide Rating",
       title = "Scatterplot of relationship between Q Scores and Weekly Workload")
```

```{r, warning=FALSE, echo=FALSE, fig.cap="Q Guide Scores at Harvard", fig.height=4.5}
# Define orange box
# EG: I need to fix this later- how should I define the box?
margin_x <- 0.15
margin_y <- 0.075
box <- tibble(
  x = c(7.83, 8.17, 8.17, 7.83, 7.83) + c(-1, 1, 1, -1, -1) * margin_x,
  y = c(4.6, 4.6, 5, 5, 4.6) + c(-1, -1, 1, 1, -1) * margin_y
  )

qscores_ch10 %>%
  ggplot(aes(x = hours, y = rating)) +
  geom_point() +
  labs(x = "Hours of Work Per Week", 
       y = "Q Guide Rating",
       title = "Scatterplot of relationship between Q Scores and Weekly Workload") +
  geom_path(data = box, aes(x=x, y=y), col = "orange", size = 1)
```
<!-- EG: Need to be more precise with this in the future. Will come back and edit later. -->

Observe that most courses have reported average workloads between 2 and 10 hours per week, while most teaching scores lie between 3 and 5. Furthermore, while opinions may vary, it is our opinion that the relationship between Q guide rating and weekly workload in hours is "weakly negative." This is consistent with our earlier computed correlation coefficient of `r cor_ch10`.

<!-- EG: I'll change this language below as well once I figure out how to work on the box. -->

Furthermore, there appear to be six points in the top-right of this plot highlighted in the box. However, this is not actually the case, as this plot suffers from *overplotting*. Recall from Subsection \@ref(overplotting) that overplotting occurs when several points are stacked directly on top of each other, making it difficult to distinguish them. So while it may appear that there are only six points in the box, there are actually more.  This fact is only apparent when using `geom_jitter()` in place of `geom_point()`. We display the resulting plot along with the same small box as before.

```{r, eval=FALSE}
qscores_ch10 %>%
  ggplot(aes(x = hours, y = rating)) +
  geom_jitter() +
  labs(x = "Hours of Work Per Week", y = "Q Guide Score",
       title = "Scatterplot of relationship between Q Scores and Weekly Workload")
```

```{r, warning=FALSE, echo=FALSE, fig.cap="Q Guide Scores at Harvard.", fig.height=4.2}
qscores_ch10 %>%
  ggplot(aes(x = hours, y = rating)) +
  geom_jitter() +
  labs(x = "Hours of Work Per Week", y = "Q Guide Score",
       title = "Scatterplot of relationship between Q Scores and Weekly Workload for Music Classes") +
  geom_path(data = box, aes(x = x, y = y), col = "orange", size = 1)
```

<!-- EG: I'll change this section as well after changing the box size. -->

It is now apparent that there are ?? points in the area highlighted in the box and not six as originally suggested. Recall from Subsection \@ref(overplotting) on overplotting that jittering adds a little random "nudge" to each of the points to break up these ties. Furthermore, recall that jittering is strictly a visualization tool; it does not alter the original values in the data frame `qscores_ch10`. To keep things simple going forward, however, we'll only present regular scatterplots rather than their jittered counterparts.

Let's build on the unjittered scatterplot by adding a "best-fitting" line: of all possible lines we can draw on this scatterplot, it is the line that "best" fits through the cloud of points. We do this by adding a new `geom_smooth(method = "lm", se = FALSE)` layer to the `ggplot()` code that created the scatterplot. The `method = "lm"` argument sets the line to be a "`l`inear `m`odel." The `se = FALSE` \index{ggplot2!geom\_smooth()} argument suppresses _standard error_ uncertainty bars. (We defined the concept of _standard error_ in Subsection \@ref(sampling-definitions).)

```{r, warning=FALSE, fig.cap="Regression line."}
qscores_ch10 %>%
  ggplot(aes(x = hours, y = rating)) +
  geom_jitter() +
  labs(x = "Hours of Work Per Week", y = "Q Guide Score",
       title = "Scatterplot of relationship between Q Scores and Weekly Workload for Music Classes") + 
  geom_smooth(method = "lm", se = FALSE)
```

The line in the resulting figure is called a "regression line." The regression line \index{regression!line} is a visual summary of the relationship between two numerical variables, in our case the outcome variable `rating` and the explanatory variable `hours`. The positive slope of the blue line is consistent with our earlier observed correlation coefficient of `r cor_ch10` suggesting that there is a negative relationship between these two variables: as students report higher average weekly workloads for music classes, courses receive lower teaching evaluations. We'll see later, however, that while the correlation coefficient and the slope of a regression line always have the same sign (positive or negative), they typically do not have the same value.

Furthermore, a regression line is "best-fitting" in that it minimizes some mathematical criteria. We present these mathematical criteria in Section \@ref(leastsquares), but we suggest you read this subsection only after first reading the rest of this section on regression with one numerical explanatory variable.

### Interpreting correlation with lm() {#model1table}

<!-- DK: Make this with just one decimal. Indeed, can we make that consistent everywhere? Also, hours should be renamed beauty. We don't really care if it is an average. 

The train example? With att_start and att_end? Or some other data set? Which one? Or qscores?
-->

You may recall from secondary/high school algebra that the equation of a line is $y = a + b\cdot x$. (Note that the $\cdot$ symbol is equivalent to the $\times$ "multiply by" mathematical symbol. We'll use the $\cdot$ symbol in the rest of this book as it is more succinct.) It is defined by two coefficients $a$ and $b$. The intercept coefficient $a$ is the value of $y$ when $x = 0$. The slope coefficient $b$ for $x$ is the increase in $y$ for every increase of one in $x$. This is also called the "rise over run."

However, when defining a regression line, we use slightly different notation: the equation of the regression line is $\hat{y} = b_0 + b_1 \cdot x$ \index{regression!equation of a line}. The intercept coefficient is $b_0$, so $b_0$ is the value of $\hat{y}$ when $x = 0$. The slope coefficient for $x$ is $b_1$, i.e., the increase in $\hat{y}$ for every increase of one in $x$. Why do we put a "hat" on top of the $y$? It's a form of notation commonly used in regression to indicate that we have a \index{regression!fitted value} "fitted value," or the value of $y$ on the regression line for a given $x$ value. We'll discuss this more in the upcoming Subsection \@ref(model1points).

<!-- DK: Is this a good introduction to hat notation? This stikes me as a subtle point that should be wesved throughout the book.  -->

<!-- EG: Honestly don't think that this introduction is bad, but certainly think that it could happen much earlier in the book. Many students may already be familiar with hat notation and it is key to regression language. -->

We know that the regression line we plotted has a negative slope $b_1$ corresponding to our explanatory $x$ variable `hours`. Why? Because music courses with higher `hours` scores tend to have lower Q Guide evaluation `scores`. However, what is the numerical value of the slope $b_1$? What about the intercept $b_0$? As we have discussed in the previous chapters, we can obtain the values of the intercept $b_0$ and the slope for `scores` $b_1$ in two steps:

1. "Fit" the linear regression model using the `lm()` function and save it in `score_model`. We put the name of the outcome variable on the left-hand side of the `~` "tilde" sign, while putting the name of the explanatory variable on the right-hand side. This is known as R's \index{R!formula notation} *formula notation*.
1. Apply the `tidy()` \index{broom!tidy()} function from the **broom** package to `score_model`. This will create the regression table.

```{r, eval=FALSE}
# Fit regression model:

score_model <- lm(rating ~ hours, data = qscores_ch10)

# Get regression table:

score_model %>% 
  tidy(conf.int = TRUE)
```
```{r, echo=FALSE}
score_model <- lm(rating ~ hours, data = qscores_ch10)
evals_line <- score_model %>%
  tidy() %>%
  pull(estimate)
```
```{r, echo=FALSE, fig.cap="Linear regression table"}
tidy(score_model,
     conf.int = TRUE) 
```

The intercept $b_0$ = `r rating_line[1]` is the average Q guide rating $\widehat{y}$ = $\widehat{\text{rating}}$ for those music courses where the course had an average weekly workload `hours` of 0. Or in graphical terms, it's where the line intersects the $y$ axis when $x$ = 0. Note, however, that while the \index{regression!equation of a line!intercept} intercept of the regression line has a mathematical interpretation, it has no *practical* interpretation here, since observing a class with a workload of 0 `hours` is impossible: the summary statistics we examined earlier showed us that students reported a minimum music class workload of at least 2.5 hours per week. Furthermore, looking at the scatterplot with the regression line, no courses had a weekly workload near 0 hours.

Remember when thinking about this equation of a regression line what we have learned in previous chapters about hat notation. Similar to $\widehat{p}$ which we have estimated and discussed before, $\widehat{y}$ is also an estimate of a potential outcome. It is not a variable representative of something we can observe in the real world" It is only an estimate. Thus, we must be cautious about extrapolating too much from $\widehat{y}$. In contrast, notice that $x$ does not have a hat. This is an indication that $x$ is not an estimate of a possible outcome but rather represents actual data. It is something that we can observe in the real world.

Of greater interest is the \index{regression!equation of a line!slope} slope $b_1$ = $b_{\text{hours}}$ of `r rating_line[2]`. The "hours" subscript indicates that this number summarizes the relationship between the Q guide rating and weekly workload variables. Note that the sign is negative, suggesting a negative relationship between these two variables, meaning courses with higher `hours` scores tend to have lower Q guide ratings. Recall from earlier that the correlation coefficient is `r cor_ch10`. They both have the same negative sign, but have a different value. Recall further that the correlation's interpretation is the "strength of linear association". The \index{regression!interpretation of the slope} slope's interpretation is a little different:

> For every increase of 1 unit in `hours`, there is an *associated* decrease of, *on average*, `r rating_line[2]` units of `rating`.

We say that this associated decrease is *on average* `r rating_line[2]` units of teaching `rating`, because you might have two music courses that have `hours` scores differing by 1 unit, but their difference in Q guide ratings won't necessarily be exactly `r rating_line[2]`. What the slope of `r rating_line[2]` is saying is that across all possible courses, the *average* difference in Q guide rating between two courses with weekly workloads that differ by one hour is `r rating_line[2]`.

Furthermore, we only state that there is an *associated* decrease and not necessarily a *causal* decrease. For example, perhaps it's not that higher amounts of weekly work in music classes directly cause lower Q guide ratings per se. Instead, the following could hold true: students may be spending extensive hours on a course every week because the instructor holds very few office hours and thus students struggle to get through the material easily. The lower Q guide rating could be due to the fact that the instructor did not hold many office hours and students wanted to discuss class material, not simply because the course took more hours per week. In other words, just because two variables are strongly associated, it doesn't necessarily mean that one causes the other. This is summed up in the often quoted phrase, "correlation is not necessarily causation."

In fact, there are many possible explanations for why a student might have given a course a lower Q guide evaluation that have nothing to do with hours of work per week. The course might not have covered the material which the student was expecting it to, causing the student to dislike the course. It might have been a lecture-style class which the student found boring. The professor of the course might have been poor at explanations of important concepts, or could have been unclear about many assignments, or could have graded with excessive harshness. The point is, there are many, many possibilities for why a student might have disliked a class and given it a lower Q guide rating besides simply that the course took up more hours per week. A course taking 12 hours of work per week might not have mattered at all to a student- any of these other factors could have been at play.

<!-- Should we cut this section about the doctor if we've already basically discussed confounding variables above and just move the actual language of "confounding variables" to the Q scores example? Or leave it? -->

Consider another example: a not-so-great medical doctor goes through medical records and finds that patients who slept with their shoes on tended to wake up more with headaches. So this doctor declares, "Sleeping with shoes on causes headaches!"

```{r, echo=FALSE, fig.cap="Does sleeping with shoes on cause headaches?"}
knitr::include_graphics("10-continuous-response-i/images/shoes_headache.png")
```

However, there is a good chance that if someone is sleeping with their shoes on, it's potentially because they are intoxicated from alcohol. Furthermore, higher levels of drinking leads to more hangovers, and hence more headaches. The amount of alcohol consumption here is what's known as a *confounding* variable\index{confounding variable}. It lurks behind the scenes, confounding the causal relationship (if any) of "sleeping with shoes on" with "waking up with a headache." We can summarize this with a *causal graph* where:

* Y is a *response* variable; here it is "waking up with a headache." \index{variables!response / outcome / dependent}
* X is a *treatment* variable whose causal effect we are interested in; here it is "sleeping with shoes on."\index{variables!treatment}

```{r, echo=FALSE, fig.cap="Causal graph."}
knitr::include_graphics("10-continuous-response-i/images/flowchart.009-cropped.png")
```

To study the relationship between Y and X, we could use a regression model where the outcome variable is set to Y and the explanatory variable is set to be X, as you've been doing throughout this chapter. However, the causal graph also includes a third variable with arrows pointing at both X and Y:

* Z is a *confounding* variable \index{variables!confounding} that affects both X and Y, thereby "confounding" their relationship. Here the confounding variable is alcohol.

Alcohol will cause people to be both more likely to sleep with their shoes on as well as be more likely to wake up with a headache. We can see why this is a problem under the potential outcomes framework: both whether you receive the treatment "sleeping with shoes on" and your potential outcome depend on whether you consumed alcohol, which means that you can't estimate the treatment effect of "sleeping with shoes on" simply by comparing the observed outcome under treatment and the observed outcome under control.  Thus any regression model of the relationship between X and Y should also use Z as an explanatory variable. In other words, our doctor needs to take into account who had been drinking the night before. In the next chapter, we'll start covering multiple regression models that allow us to incorporate more than one variable in our regression models.

<!-- DK: In the next chapter? Not if we re-organize. -->

Establishing causation is a tricky problem and frequently takes either randomized controlled trials or methods to adjust for the effects of confounding variables. Both these approaches attempt, as best they can, either to take all possible confounding variables into account or negate their impact. This allows researchers to focus only on the relationship of interest: the relationship between the outcome variable Y and the treatment variable X.

As you read news stories, be careful not to fall into the trap of thinking that correlation necessarily implies causation.  Check out the [Spurious Correlations](http://www.tylervigen.com/spurious-correlations) website for some rather comical examples of variables that are correlated, but are definitely not causally related.

<!--AR: trying to be precise that it is correlation with the treatment and *potential* outcomes that matters, not correlation with the treatment and observed outcomes.  What's a better way to explain this?
DK: Agreed! Indeed, we need to hit the notion of potential outcomes each chapter. Potential outcomes != observed outcomes! Not mentioning RCM for 4 weeks is a mistake. -->   

Let's say, however, that we were confident that there is no confounding: that is, that there are no variables such as socioeconomic background that correlate both with a teacher's beauty score and the teacher's potential outcomes. (We shouldn't be confident in this, but let's play along for the moment.)  Then, we can interpret the slope in terms of the [Rubin Causal Model](#rubin-causal-model).  The slope coefficient on `hours` of `r evals_line[2]` then means that the *average treatment effect* of increasing a teacher's "beauty" score by 1 is `r evals_line[2]`.  In the absence of randomization, however, this is likely not a good interpretation of this regression!  Adding additional variables, as we'll do in Chapter \@ref{multiple-regression}, may make it more plausible to interpret the regression causally.

### Introduction to Bayesian regression

Thus far, we have taken our simple `lm()` model and interpreted it in a Bayesian way. However, is this a full-fledged Bayesian analysis? No! As mentioned before, `lm()` is an old function and to perform Bayesian analysis we must use a Bayesian analysis tool. Such a tool will allow us to more effectively integrate knowledge of the probability of an event before new observations are collected (priors) and to more easily explore revised probabilities following the collection of new data (posteriors).

<!-- EG: Do students know what Bayesian regression is? How it differs fundamentally from frequentist modeling? Why we might consider using a Bayesian model instead of a frequentist model? If that hasn't already been introduced somewhere else in the book, I think it should happen here. -->

The tool we need to perform Bayesian Regression is the `rstanarm` package. `rstanarm` is an interface to connect with the Stan probabilistic programming language. <!-- EG: should we have some explanation of what the Stan probabilistic programming language is?-->You will see the `rstanarm` package is created to mirror functions like `lm()` and it simply requires adding a `stan_` prefix before common functions like `lm()`. We will focus on `stan_glm()` for the remainder of this section. 

## Bayesian Regression with a Continuous Variable

Let us take another look at the the data on music courses at Harvard which we have been investigating in this chapter.  We will once again model the relationship between Q Guide ratings and average weekly workload to help us highlight the similarities and differences between `lm()` and `stan_glm()`. 

We can obtain the values of the intercept $b_0$ and the slope for `rating` $b_1$ in two steps:

1. "Fit" the Bayesian regression model using `stan_glm()` function and save it in `bayes_score`. 
1. Applying the `print()` function and indicating how many significant digits to include.

```{r}

library(rstanarm)

bayes_score <- stan_glm(rating ~ hours, data = qscores_ch10, refresh = 0)

print(bayes_score, digits = 2)


```

First, we "fit" the bayesian regression model to the `data` using the `stan_glm()` \index{lm()} function and saved this as `bayes_score`. Notice `stan_glm()` is used just as `lm()` was: `stan_glm(y ~ x, data = data_frame_name)` where:

* `y` is the outcome variable, followed by a tilde `~`.
* `x` is the explanatory variable.
* The model formula is `rating ~ hours`.
* `data_frame_name` is the name of the data frame that contains the variables `y` and `x`. In our case, `data_frame_name` is the `qscores_ch10` data frame.
* `refresh = 0` is optional. Setting refresh equal to 0 suppresses the printing of the sampling algorithm from the model and can make the function run more quickly. We will touch more on this later. 

Recall from our discussion in Chapter \@ref(tidyverse) that we can use "." to refer to the tibble that has been passed in by the proceeding pipe. In this case, that is the `qscores_ch10` tibble. Using "." to refer to the passed-down tibble will be a trick which we use again and again in the coming chapters.

Notice, in the `print()` output, the estimates are referred to as the `Median` and the uncertainty as the `MAD_SD`. This is due to the fact that Bayesian Regression does not provide just a point estimate, but a distribution. The `Median` is the chosen summary statistic for the distribution because median-based-summaries are more stable over simulations. However, even though the estimates and uncertainty of the parameters are calculated differently, the coefficients of the `stan_glm()` model can be intepretted in the same manner as they were previously when using `lm()`. 

The intercept $b_0$ = `r bayes_score$coefficient[1]` is the average Q Guide score $\widehat{y}$ = $\widehat{\text{rating}}$ for a music course where the weekly workload `hours` was 0. Again, while the \index{regression!equation of a line!intercept} intercept of the regression line has a mathematical interpretation, it has no *practical* interpretation here, since observing a workload of 0 `hours` is impossible.

<!-- DK: Add discussion of "hat". Recall the p hat which we estimated last time. y hat is like that! It is not a variable that we can observe. It is an estimate. (Indeed, it is an estimate of a potential outcome!) This is different form x, which has no hat, because it is real data, something we can see. Side note: not sure if the hat versus no hat distinction works well with b_0/b_1 being things we can't see and need to estimate, but for which we do not use hat notation. -->

The slope $b_1$ = $b_{\text{hours}}$ of `r bayes_score$coefficient[2]`. The "hours" subscript indicates that this number summarizes the relationship between the Q Guide rating and average weekly workload variables. The slope's interpretation is different:

<!-- DK: Let's make sure that these interpretations are highly consistent with Gelman and across the book. -->

<!-- EG: To make sure these interpretations are consistent, who should I talk to/what sections should I look at? -->

> For every increase of 1 unit in `hours`, there is an *associated* increase of, *on average*, `r bayes_score$coefficient[2]` units of `rating`.

What the slope of `r bayes_score$coefficient[2]` is saying is that across all possible music courses, the *average* difference in Q Guide score between two music courses where weekly workloads differ by one hour is `r bayes_score$coefficient[2]` when holding all other things equal.

In the output, we are also given another parameter, $sigma$ =  `r round(sigma(bayes_score),2)`. $sigma$ represents the residual standard error, the deviation between the predicted value and the observed value. Thus, it shows us the uncertainity when our model predicts the `score` using `hours`. Our $sigma$ 
tells us that a music course's Q Guide rating will be between plus or minus `r round(sigma(bayes_score),2)` the prediction for 68% of the data and between plus or minus two sigma, `r 2*round(sigma(bayes_score),2)`, of the prediction for about 95% of the data. Here is a visual representation:

```{r, echo=FALSE}

qscores_ch10 %>%
  add_predicted_draws(bayes_score) %>%
  ggplot(aes(x = bty_avg, y = score)) +
  stat_lineribbon(aes(y = .prediction), .width = c(.99, .95, .68), color = "#08519C") +
  geom_point(data = evals_ch11, size = 2) +
  scale_fill_brewer() + 
  labs(title = "68% of Data Between +-1 Sigma from Line of Best Fit\n 95% of Data Between +-2 Sigma from Line of Best Fit")
```


```{r, echo=FALSE}
intervals<-posterior_interval(bayes_score, prob = 0.95)

```

Finally, for all of the coefficients, we are provided a `MAD_SD`. The `MAD SD`, which equals $1.483 * (\mathbf{median}^n_{i=1} |z_i - M|)$, summarizes the uncertainty in the model parameters. Since we are used to using the standard deviations to measure variation, the MAD SD is rescaled to mirror the standard error of the normal distribution. The `MAD_SD` can be used to retrieve our coefficient's credible intervals for each parameter. Thus, for `hours`, the 95% credible interval is (`r intervals[2,]`). This tells us that there is a 95% probability that the true value of the parameter for `hours` falls within the interval. 

<!-- EG: Should we include an explanation of why the language of posteriors is so important/so heavily used within Bayesian modeling? -->

Instead of trying to calcualate these by hand, the easiest way to retrieve our model's credible intervals is using the function `posterior_interval()` on our model object `bayes_score`.

```{r}
posterior_interval(bayes_score)

```

The default for the function is a 90% interval, but that can be changed by adding the `prob` input. Thus, we can replicate the 95% intervals that we have worked with in the past:

```{r}
posterior_interval(bayes_score, prob = 0.95)

```

Now, let us call `summary()` on our `bayes_score` model. Using the `summary()` is an alternative to `print` and it provides more information about our `stan_glm()` model. 

```{r}
summary(bayes_score)
```

In the Estimates section, we get the same information as we did from `print()`. However, there is a lot of new information that we have not seen before under Fit Diagnostics and MCMC Diagnostics. We can use this information provided to further assess the model. 

In the Fit Diagnostics section, we are provided information about the `mean_ppd`, the sample mean of the posterior predictive distribution of the outcome variable. A quick check of our model is that we hope the `mean_ppd` is close to the mean of our depedent variable. The mean of the `score` variable is `r mean(qscores_ch10$rating)`. Thus, the `mean_ppd` is definitely on par with the mean of the `bayes_score` dependent variable. 

In MCMC diagnostics, we are provided with:

* `log-posterior`, which is the log of the combined posterior distributions. 
* `mcse`, which stands for the Monte Carlo standard error. Markov Chain Monte Carlo is the algorithm used to draw from the posterior distribution.
* `Rhat`, which indicates whether or not the model converges.
* `n_eff`, a measure of the effective sample size.

The only thing that you need to be concerned about is the `Rhat`. When we know whether or not the model converges, we know whether or not the results are reliable. We hope to get values for `Rhat` as close to 1 as possible. Rhat values less than 1.1 indicate model convergence and that the model is reliable. Thus, since all the `Rhat` for our model are 1.0, `bayes_score` is reliable.


## Advanced: Bayesian Regression 
## Introduction to rstanarm

### Interpreting Regression Coefficients 

### Uncertainty in Bayesian Inference
You may be wondering, what exactly is `stan_glm()` doing? Let us take a look under the hood of `stan_glm()`. This will enable us to further understand the usefulness of Bayesian Regression. Remember that we included `refresh = 0` in the `bayes_score` model. Let us run the same model, but leave out the `refresh` input.

```{r}
stan_glm(score ~ bty_avg, data = evals_ch11)
```

Without `refresh`, the output displays everything `stan_glm()` is doing in the background, which is a sampling algorithm known as Markov Chain Monte Carlo (MCMC). In fact, `stan_glm()` is producing thousands of simulations after sampling from the posterior distribution using this MCMC algorithm. We can conveniently access all of the simulations of the the model parameters (intercept, slope, and sigma) from the posterior distribution as a matrix. Let's take a look at some of these simulations.

```{r}

sims <- as.matrix(bayes_score)


head(sims)
nrow(sims)
``` 

As you can see, each row or iteration has a slightly different value for our `bayes_score` model's three parameters. Each combination of possible values of the parameters was used to try to fit the data. Also, looking at the dimensions of the matrix, there are 4000 rows. Thus, `stan_glm()` tried `4000` different combinations of possible values of the parameters to try to model the data. 

Now, you may be wondering what is going on because there are `4000` simulations, but when we print the model, we get a single value for each parameter. Recall that what `lm()` refers to as the estimate, when we printed our `bayes_score` model, the column was headed as `Median`. Let's calculate the median value for each column of our `sims` matrix. We can do this using the `apply()` function, which takes form: `apply(Object, Margin, Function)`. We will be applying the `median` function to the `sims` matrix and the `Margin` will be 2, which indicates columns instead of rows.

```{r}
apply(sims, 2, median)
```

These are the exact values of the coefficients of the printed `bayes_score` model. Thus, `stan_glm()` tried 4000 iterations to model the data. All `4000` were summarized using the `Median` to produce point estimates of the parameters.

Now, we understand where the point-estimate comes from when we call `print()` on a `stan_glm()` model; however, a simple linear regression using `lm()` provides us with a point-estimate, or in other words,  a single line of best fit. The power of Bayesian Regression comes from the ability to analyze uncertainty using the entire posterior distribution, all 4000 simulations cumulatively. Again, the posterior distribution is a set of plausible values for each parameter and each observation or row is referred to as a posterior draw. Let us take a look at the posterior distrubtion for the `bty_avg` variable, the slope, by graphing it. Once again we can use the `sims` matrix, but we must convert it into a tibble using `as_tibble()` to graph. 

Let us take a look at the posterior distrubtion for the `hours` variable, the slope, by graphing it. Once again we can use the `sims` matrix, but we must convert it into a tibble using `as_tibble()` to graph. 


```{r, message= FALSE, echo=FALSE}
library(bayesplot)
library(tidybayes)
library(cowplot)
```

```{r}



posterior_draws<-sims%>%as_tibble()


posterior_draws%>%
  ggplot(aes(x = hours)) +
  geom_histogram()+
   geom_vline(xintercept=median(posterior_draws$hours), color="red", size=1) + labs(y = "Frequency", title = "Posterior Distribution of hours Parameter", subtitle = "The Red Line Represents the Median")
```

If you noticed, `stan_glm()` does not have any p-values, t-values or degrees of freedom like `lm()`. The crux of Bayesian Modeling is that everything we need to know can be found within the posterior distribution. We can get a point estimate using the `Median` and we saw in the previous section that we can get a 95% credible interval for `hours` using the `posterior_interval` function. We can also graph the posterior distribution of all the other parameters from the model, the intercept and sigma. 

```{r echo=FALSE}

hours<-posterior_draws%>%
  ggplot(aes(x = hours)) +
  geom_histogram()+
   geom_vline(xintercept=median(posterior_draws$hours), color="red", size=1) + labs(y = "Frequency", title = "Posterior Distribution of hours Parameter", subtitle = "The Red Line Represents the Median")

sigma<-posterior_draws%>%
  ggplot(aes(x = sigma)) +
  geom_histogram()+
   geom_vline(xintercept=median(posterior_draws$sigma), color="red", size=1)

intercept<-posterior_draws%>%
  ggplot(aes(x = `(Intercept)`)) +
  geom_histogram()+
   geom_vline(xintercept=median(posterior_draws$`(Intercept)`), color="red", size=1)


cowplot::plot_grid(intercept, hours, sigma, nrow = 1)



    

```

Another way to think of each posterior draw, each row of the `sims` matrix, is that each creates a regression line. This is something that may get lost in translation with the posterior distrubution and the 4000 simulations; however, the goal of `stan_glm()` is still to fit lines to data. 

```{r}

medians<-tibble(`(Intercept)` = median(posterior_draws$`(Intercept)`), hours = median(posterior_draws$hours))

posterior_draws%>%
  ggplot(aes(x = `(Intercept)`, y = hours)) +
  geom_point() + geom_point(data = medians, color = "red", size = 5)+ 
  labs(title = "Every Combination of Intercept and hours From Posterior Distribution", x = "Intercept", subtitle = "Red Dot Represents the Median for Each Parameter")

```
Each intercept and slope (hours) combination will create a different line. Furthermore, we can view all of these various lines. In doing so, we can again express and understand uncertainty.

We will take our first look at the **tidybayes** package. The purpose of the **tidybayes** package is to aid in formating Bayesian modeling outputs in a tidy manner. It also provides **ggplot** geoms to easily plot Bayesian Models.

Now back to the matter at hand of the various posterior draws and the lines they create. If you recall the `augment()` function we used for `lm()`, we were able to retrieve the fitted values yˆ. The `add_fitted_draws()` function from the **tidybayes** package is very similar, but for `stan_glm()`. The `add_fitted_draws()` takes a `stan_glm()` object, such as `bayes_score`, and a `n` parameter, which stands for the number of posterior draws to calculate the fitted values for. Let us try `n=5`.

```{r}
five_draws<-qscores_ch10%>%
  add_fitted_draws(bayes_score, n = 5)

nrow(five_draws)

```
If you recall, the `qscores_ch10` dataset has 463 rows. `2315/5 = 463`. Thus, we can see that `add_fitted_draws()` calculated the fitted values 5 times. Let us see what `five_draws` looks like.

```{r}
head(five_draws, n = 10)
```

`add_fitted_draws`reports:

* `ID` and `.row` which correspond to the row or specific teacher from the `qscores_ch10` dataset.
* `score`, `hours` and `age` are the observed values for a given teacher. 
* `.draw` is the randomly selected posterior draw used to fit the data. The number corresponds to the row from the `sims` matrix used. 
* `.value` is the fitted value. In this case, it represents the prediction for the `score` based on a teacher's `hours` for the specific parameters from the `.draw`. 

We are able to plot these different draws.

```{r}

five_draws%>%
  ggplot(aes(x = hours, y = score)) +
  geom_line(aes(y = .value, group = .draw), color = "blue") +
  geom_point()


```

Now, you can see how each posterior draw creates a different line. With just these 5 draws, there is a lot of variability. Let us see what `n=100` looks like.

```{r}
qscores_ch10%>%
  add_fitted_draws(bayes_score, n = 100)%>%
  ggplot(aes(x = hours, y = score)) +
  geom_line(aes(y = .value, group = .draw), alpha = 0.6, color = "blue") +
  geom_point()
  #transition_states(.draw, 0, 1)+
  #shadow_mark(past = TRUE, future = TRUE, alpha = 1/20, color = "gray50")
```
We see a the lines become more concentrated. The power of Bayesian Modeling comes from the shear number of draws. Now remember, `stan_glm()` by default produces 4000 simulations and we have only plotted 100 of them. The more draws there are, the better your estimation of the posterior distribution and the better we are able to assess the uncertainty in our parameters. 


###Predictions

Finally, the last step of Bayesian Inference is being able to make prediction about new data using our `stan_glm()` model. Let us create a new dataset of teacher's beauty scores that mirrors the range of values in the `eval_ch11` dataset. To do this, we will create a tibble of `hours` and use `seq` to generate a row for each `hours` from 1.5 to 8.5 by 0.5. 

```{r}

new <- tibble(hours = seq(1.5, 8.5, 0.5))

new
```

Now, we will start with the simplest prediction, the point prediction. These are based on the fitted model or as we have come to know, the median value for each parameter (intercept, hours, sigma). We will use the `predict` function to find predicted scores for each beauty rating from our `new` dataset. 
```{r}

new%>%
  mutate(y_hat = predict(bayes_score, newdata = .))%>%
  ggplot(aes(x = hours, y = y_hat)) + geom_point() + geom_line()


```

Great! We have predicted values for each  `hours` from the `new` dataset and they follow line created by the fitted model. However, this is nothing special to `stan_glm()`. We can do the exact same thing with an `lm()` model. Recall the `score_model` we created at the beginning of Chapter 10. It is the same as the `bayes_score` model, but it uses `lm()` instead of `stan_glm()`. Here are the point predictions for the `score_model` at each value of `hours`in the `new` dataset.

```{r}
new%>%
  mutate(y_hat = predict(score_model, newdata = .))%>%
  ggplot(aes(x = hours, y = y_hat)) + geom_point() + geom_line()
```

We get the exact same thing. This is because a point prediction ignores uncertainty. As we have been exploring in this section, where Bayesian Regression differs from Simple Linear Regression is in it's ability to express uncertainty in not only the model, but also it's parameters. Remember we had a `sims` matrix of 4000 rows of parameters. The posterior distributions serves us better and gives us a lot more information than a single point estimate ever could. 

Thus, our predicitions should also take uncertainty into account. The first type is linear predictors. We perform this type of prediction using the `posterior_linepred` function. `posterior_linepred` takes a stan model like our `bayes_score` and a dataset of new points, `new`. With the function, we will be able to represent the distribution of uncertainty in regards to the parameters at each `hours` from the `new` dataset. Let us take a look.

```{r}
linepred<-posterior_linpred(bayes_score, newdata = new)



head(linepred)
```

`posterior_linepred` returns a matrix of posterior simulations wher each column of the `linepred` corresponds to each row of the `new` dataset. If we were to take the `mean` of each column, we would get the point prediction corresponding to that `hours`. Also, if we were to take the `sd`, you would get the uncertainty in the fitted model. Thus `posterior_linepred` mimics what we were doing the previous section with the matrix of simulated parameters. In fact, we plot `posterior_linepred` using the same `add_fitted_draws` function without specifying n to factor in all 4000 posterior draws. `added_fitted_draws` from the **tidybayes** package puts the data in a much easier format than the matrix `posterior_linepred` outputs. First, we will recreate the point prediction plot using `add_fitted_draws` and use `mutate` to get the mean prediction for each `hours`. 

```{r}

new %>%
  add_fitted_draws(model = bayes_score, .)%>%
  group_by(hours)%>%
  mutate(point_estimate = mean(.value))%>%
  ggplot(aes(x = hours, y = point_estimate)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq(1.5,8.5,0.5)) +
  labs(y = "Predicted Scores", color = "Uncertainty Level", title = "Posterior Linear Prediction Without Uncertainty")


    


```

Now, we will add to this plot using the `stat_interval` geom from the **tidybayes** package to add uncertainty to the visualization.

```{r}
new %>%
  add_fitted_draws(model = bayes_score, .)%>%
  group_by(hours)%>%
  mutate(point_estimate = mean(.value))%>%
  ggplot(aes(x = hours, y = point_estimate)) +
  stat_interval(aes(x = hours, y = .value), .width = c(0.68, 0.95)) +
  geom_point(aes(x = hours, y = point_estimate)) +
  geom_line() +
  scale_x_continuous(breaks = seq(1.5,8.5,0.5))+
  labs(y = "Predicted Scores", color = "Uncertainty Level", title = "Posterior Linear Prediction With Uncertainty")
```

Linear Predictions can provide us the point estimate predictions while also telling us about the uncertainty in the parameters. As you can see, the intervals get a lot smaller towards the center of the `hours` variable. This is because more data is concentrated towards the center. Furthermore, the predictions are more accurate/less varied because the posterior distribution has more data to draw from. Towards the extremes, we see the bands are a lot wider. Thus, the point predictions are a lot less reliable because the posterior distribution does not have a lot of data from the original `qscores_ch10` to be built off of. 

Finally, the last type of prediction we will cover is the predictive distribution. Whereas linear predictions focused on the uncertainty in the parameters coefficients, the predictive distribution represents uncertainty surrounding the predicted value. In other words, it "is the distribution of the outcome implied by the model after using the observed data to update our beliefs about the unknown parameters in the model." To perform this, we will use the `posterior_predict()` function. It works in the same way as `posterior_linpred`. 

```{r}

pred<-posterior_predict(bayes_score, newdata = new)
  
head(pred)

```

Again, in the matrix that is returned, each column corresponds to one new value of  `hours` from the `new` dataset. We can take a quick look at the predictive distribution for one of the `hours` using the `hist()` function. 
```{r}
hist(pred[,1])
```

However, **tidybayes** also has a function to make our lives easier for graphing the predictive distributions where we can compare all the distributions cumulatively in one plot. First, we have the `add_predicted_draws()` function which does the same thing as `posterior_predict()`, but `add_predicted_draws()` puts the data in a tidy format. 


```{r}

new %>%
  add_predicted_draws(., model = bayes_score)%>%
  ggplot(aes(x = .prediction, y = hours)) +
  stat_halfeyeh() + 
  scale_y_continuous(breaks = seq(1.5,8.5,0.5)) +
  labs(x = "Predicted Score", title = "Predictive Distribution for Score at Each Value of hours")
```

With `stat_halfh()` we are able to view density plots for all the predictions from the `new` dataset together. Thus, we can compare all the predictive distributions. As you can see, there is a lot of overlap. This signifies there is a lot of uncertainty. Even though the  `hours` has a slightly postive slope, `hours` does not seem to be very predictive when it comes to the teacher's `score`.


## Bayesian Regression with Categorical Variable 

In the previous section we focused on Bayesian Regression with one continuous variable. We will now take a look at a model wtih one categorical variable. We will follow many of the same steps as we did in the previous section, thus we will go through the steps of Bayesian Inference quicker in the section while highlighting the differences when regressing on a categorical variable. We will use the same data from the `gaminder` package on the 142 countries from 2007. 

```{r}
library(gapminder)

gapminder2007 <- gapminder %>%
  filter(year == 2007) %>%
  select(country, lifeExp, continent, gdpPercap)

head(gapminder2007)
```

As we did before, we will study the relationship between continents and life expectancy. We will “fit” the bayesian regression using the `stan_glm(y ~ x, data)` function and save it in `bayes_lifeExp`.

```{r}


bayes_lifeExp <- stan_glm(lifeExp ~ continent, data = gapminder2007, refresh = 0)

print(bayes_lifeExp, digits = 2, detail = FALSE)
```

#### Interpreting Coefficients 

Remember, now that we are using a categorical explanatory variable `continent`, our model will not yield a “best-fitting” regression line, but rather offsets relative to a baseline for comparison. Remeber, also that each of these coefficients is the median of the posterior distribution for each parameter.

For example, we can take a look at the posterior distribution for the `continentAmericas` parameter. Calling `as.matrix` on the model, we can access all the posterior draws to then plot in a histogram:

```{r}
sims <- as.matrix(bayes_lifeExp)



posterior_draws<-sims%>%as_tibble()


Americas<-posterior_draws%>%
  ggplot(aes(x = continentAmericas)) +
  geom_histogram()+
   geom_vline(xintercept=median(posterior_draws$continentAmericas), color="red", size=1) + labs(y = "Frequency", title = "Posterior Distribution", subtitle = "The Red Line Represents the Median")

Americas

```

```{r, echo=FALSE, eval=FALSE}

# code for all the parameters posterior distributions

# posterior_draws%>%
#   pivot_longer(cols = c("(Intercept)", "sigma", starts_with("continent")), 
#                names_to = "Parameter", 
#                values_to = "draws")%>%
#   ggplot(aes(x = draws)) + 
#   geom_histogram() +
#   facet_wrap(~Parameter)

```

Now, let us break down the coefficients from the `print()` function:

```{r}
print(bayes_lifeExp, digits = 2, detail = FALSE)
```

<!-- DK: Code below does not work. -->

```{r, echo=FALSE}
# fitting the model 

bayes_lifeExp %>%
   broom.mixed::tidy() 


estimates_2 <- bayes_lifeExp %>%
  broom.mixed::tidy() %>%
  pull(estimate)
```

The coefficients can be interpreted as follows:

1. `intercept` corresponds to the mean life expectancy of countries in Africa of `r round(estimates_2[1],2)`.
1. `continentAmericas` corresponds to countries in the Americas and the value +`r round(estimates_2[2],2)` is the same as the difference in mean life expectancy relative to Africa. In other words, the mean life expectancy of countries in the Americas is `r round(estimates_2[1],2)` + `r round(estimates_2[2],2)`= `r round(estimates_2[1],2) + round(estimates_2[2],2)`.
1. `continentAsia` corresponds to countries in Asia and the value + `r round(estimates_2[3],2)` is the same as the difference in mean life expectancy relative to Africa. The mean life expectancy of countries in Asia is `r round(estimates_2[1],2) + round(estimates_2[3],2)`. 
1. `continentEurope` corresponds to countries in Europe and the value +`r round(estimates_2[4],2)` is the same as the difference in mean life expectancy relative to Africa. The mean life expectancy of countries in Europe is `r round(estimates_2[1],2) + round(estimates_2[4],2)`.
1. `continentOceania` corresponds to countries in Oceania and the value +`r round(estimates_2[5],2)` is the same as the difference in mean life expectancy relative to Africa. The mean life expectancy of countries in Oceania is `r round(estimates_2[1],2) + round(estimates_2[5],2)`.


The model also has a sigma, or residual standard error, of `r round(sigma(bayes_lifeExp),2)`. Thus, sigma tells us that a country's life expectancy will be between plus or minus `r round(sigma(bayes_lifeExp),2)` the prediction based on what continent the country is in for 68% of the data. Aslo, a country's life expectancy will be between plus or minus two sigma, `r 2*round(sigma(bayes_lifeExp),2)`, of the prediction for about 95% of the data given which continent the country is in.

Finally, for all of the coefficients, we are provided a MAD_SD. We can use it to create each coefficient's credible interval. 

```{r}
posterior_interval(bayes_lifeExp, prob = 0.95)
```


Each of these tell us there is a 95% probability that the true value of each parameter will fall within the respective interval.

Now, we can call `summary()` on our `bayes_lifeExp` model:
```{r}
summary(bayes_lifeExp)
```
Again, the few things to check from summary other than tht estimates are that the `mean_ppd` is in accords with the mean of the outcome variable, in our case `life_Exp`, and that all the `Rhat`s are close to 1. The mean of the outcome variable is `r mean(gapminder2007$lifeExp)`, thus the `mean_ppd` is very close. Also all of the `Rhat`s are 1.0, signifying that our model converged and that it is reliable. 

###Prediction

Finally, we will make prediction about new data using our `stan_glm()` model. Let us create a new dataset of continents. To do this, we will create a tibble containing the five various continents from the `gaminder2007` dataset. 

```{r}

new <- tibble(continent = c("Asia", "Africa", "Americas", "Europe", "Oceania"))

new
```

We know a regression with categorical variable does not yield a line, but rather offsets relative to a baseline for comparison. Thus, `posterior_linepred()` will help us quantify uncertainty in the paramets, but it cannot be interpreted using lines. Remember, we can apply `add_fitted_draws()` instead of `posterior_linepred` because it provides the data in a tidy manner. Also, remember that the `mean` for each group of predictions, `continent`, corresponds to the point estimate. 

```{r}
new %>%
  add_fitted_draws(model = bayes_lifeExp, .)%>%
  group_by(continent)%>%
  mutate(point_estimate = mean(.value))%>%
  ggplot(aes(x = continent, y = point_estimate)) +
  stat_interval(aes(x = continent, y = .value), .width = c(0.68, 0.95)) +
  geom_point(aes(x = continent, y = point_estimate)) +
  geom_line() +
  labs(y = "Predicted Life Expectancy", color = "Uncertainty Level", title = "Posterior Linear Prediction With Uncertainty") + coord_flip()
```

From the graph, we can see variation in the point estimates for each `continent` with Africa having a signicantly lower life expectancy. From the printed output, we continentOceania had the highest coefficient, meaning the greatest difference in mean life expectancy relative to Africa. Thus, we would expect Oceania to have highest life expectancy in comparison to the other continents; however, we see their is a very wide interval. This suggests their is a lot of uncertainty in the Oceania parameter. This is perhaps due to there not being as many countries on the continent of Oceania and there are vast differences in life expectancy between the countries in the dataset. Remember that the more data their is, the more accurate our posterior predictions will be and as we can see from above, the effect of the absence of data is shown in the uncertainty in the Oceania linear predictions. 


Alternatively, we can use `posterior_predict()` to understand the posterior predicitive distribution for each continent. Remember we can perform `add_predicted_draws()` instead of `posterior_predict` because it provides the data in a tidy manner. 
```{r}


new %>%
  add_predicted_draws(., model = bayes_lifeExp)%>%
  ggplot(aes(x = .prediction, y = continent)) +
  stat_halfeyeh() + 
  labs(x = "Predicted Life Expectancy", title = "Posterior Predictive Distribution For Each Continent's Life Expectancy")

```

Unlike our predictive distribution from `bayes_score` model in the previous section, we see a lot more differentiation between the categories. Although there is overlap at the tail, African Life Expectancy appears to be significantly lower than Life Expectancy on other continents. This plot shows the predictions are more stable. We can be more assured that there are actaul differences in predicted life expectancy between continents, which is something we would not be privy to using only point predictions.

Finally, one other way to display the posterior predictive distribution is using `stat_intervalh()`. 

```{r}



new %>%
  add_predicted_draws(., model = bayes_lifeExp)%>%
  group_by(continent) %>%
  mutate(median_prediction = median(.prediction)) %>%
  ggplot(aes(x = .prediction, y = continent)) +
  stat_intervalh() + 
  geom_point(aes(x = median_prediction, y = continent)) + 
  labs(x = "Predicted Life Expectancy", title = "Posterior Predictive Distribution For Each Continent's Life Expectancy")
```
By default the uncertainty levels are `0.5,0.8, 0.95` but those can be altered with the `.width` input to your specification such as `.width = c(0.68, 0.95)`.


## Conclusion


In this chapter, you've studied the term _basic regression_, where you fit models that only have one explanatory variable. In Chapter \@ref(continuous-response), we'll study *multiple regression*, where our regression models can now have more than one explanatory variable! In particular, we'll consider two scenarios: regression models with one numerical and one categorical explanatory variable and regression models with two numerical explanatory variables. This will allow you to construct more sophisticated and more powerful models, all in the hopes of better explaining your outcome variable $y$.
